####################
#
# Example Job for HTCondor
#
####################

#---------------------------------------------
# Name your batch so it's easy to distinguish in the q.
JobBatchName = "sft_a100"

# --------------------------------------------
# Executable and its arguments
executable    = /mnt/fast/nobackup/users/ly0008/miniconda3/envs/moe_condense/bin/python

# ---------------------------------------------------
# Universe (vanilla, docker)
universe         = vanilla

# -------------------------------------------------
# Event, out and error logs
log    = c$(cluster).p$(process).log
output = c$(cluster).p$(process).out
error  = c$(cluster).p$(process).error

# -----------------------------------
# File Transfer, Input, Output
should_transfer_files = YES

# -------------------------------------
# Requirements for the Job
request_GPUs     = 1
requirements = (machine != "aisurrey09.surrey.ac.uk") && (CUDAGlobalMemoryMb > 78000) && (CUDACapability > 2.0)
# this needs to be specified for the AI@Surrey cluster if requesting a GPU
+GPUMem          = 78000
request_CPUs     = 2
request_memory   = 40G

# --------------------------------------
# Resources

#This job will complete in less than 1 hour
+JobRunTime = 32

#This job can checkpoint
+CanCheckpoint = false


arguments = $(script) --input /mnt/fast/nobackup/users/ly0008/caomingyu/transformers/datasets/mmlu.json \
        --model /mnt/fast/nobackup/users/ly0008/caomingyu/deepseek-ai/deepseek-moe-16b-base \
        --output-dir /mnt/fast/nobackup/scratch4weeks/ly0008/caomingyu \
        --max-length 2048 \
        --lr 5e-5 \
        --score-mode greedy_jl \
        --prune-num-expert 0 \
        --prune-num-layer 6 \
        --batch-size 2

script = $ENV(PWD)/moe_prune/finetune_weight_all.py

queue 1
